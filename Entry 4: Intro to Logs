### Date Completed: 04/24/2024 ###

### About

Log analysis involves collecting, parsing, and processing log files to detect security anomalies and identify system performance issues.

### Introduction

How can we identify malicious activities? What evidence is generated when an intruder breaches a network? Recognizing these indicators within our environment is essential.

Logs serve as invaluable records of past events, providing essential insights to address these questions. By preserving an archive of historical activities, we can bolster our security posture and protect our digital assets more effectively.

A comprehensive understanding of logs is crucial for identifying patterns and mitigating potential threats.

As manually examining the vast amount of log data generated by numerous systems and applications can be challenging, it is vital to grasp the intricacies of log analysis and become acquainted with the available tools and techniques.

Log analysis tools and methods empower individuals to interpret historical events and establish a reliable source of historical evidence, streamlining the processing and scrutiny of log data. This efficiency facilitates prompt detection and response to potential incidents or significant events.

By analyzing logs as records of historical activities, individuals and organizations can gain essential knowledge, enhancing their overall awareness and preparedness across a wide range of situations.

### Learning Objectives

This room covers how logs can be used to record an adversary’s actions, the tools and techniques needed to perform log analysis, and the significance of effectively collecting and analyzing logs.

- Understand the importance of logs as a historical activity record for identifying and mitigating potential threats.
- Explore various types of logs, logging mechanisms, and collection methods across multiple platforms.
- Gain hands-on experience detecting and defeating adversaries through log analysis.

### Task 2: Expanding Perspectives: Logs as Evidence of Historical Activity

#### Working with Logs: Scenario

Scenario: A web server of SwiftSpend Financial is constantly bombarded with scans from an adversary. As a systems administrator of this organization tasked to address this predicament, you must identify what the adversary is doing by configuring logging and analyzing collected logs.

#### Connecting to the Machine

Start the virtual machine in split-screen view by clicking the green Start Machine button on the upper right section of this task. If the VM is not visible, use the blue Show Split View button at the top-right of the page. Alternatively, using the credentials below, you can connect to the VM via RDP or SSH.

IMPORTANT: The attached VM contains artifacts to help us better understand logs and the implications of their analysis to the detection engineering and incident response practices. Work on the subsequent tasks and experiment with the VM through a case example. Escalation of Privileges is NOT necessary to answer the questions in this room.

#### In the Heart of Data: Logs

Just as a physical tree’s rings reveal its life story — indicating good years with thick curls and challenging ones with thin — a digital log provides a historical record of system activity.

Both embody a fundamental principle of growth over time and serve as living records in their respective domains — physical and digital.

In the digital world, every interaction with a computer system — from authentication attempts, granting authorization, accessing a file, and connecting to a network to encountering a system error — will always leave a digital footprint in the form of logs.

Logs are a record of events within a system. These records provide a detailed account of what a system has been doing, capturing a wide range of events such as user logins, file accesses, system errors, network connections, and changes to data or system configurations.

While the specific details may differ based on the type of log, a log entry usually includes the following information:

- A timestamp of when an event was logged
- The name of the system or application that generated the log entry
- The type of event that occurred
- Additional details about the event, such as the user who initiated the event or the device’s IP address that generated the event

This information is typically stored in a log file, which contains aggregated entries of what occurred at any given time on a system.

However, since digital interactions are continuous and fast-paced, the log file’s size may exponentially grow depending on the activities logged on a system.

#### The True Power of Logs: Contextual Correlation

A single log entry may seem insignificant on its own. But when log data is aggregated, analyzed, and cross-referenced with other sources of information, it becomes a potent investigation tool. Logs can answer critical questions about an event, such as:

- What happened?
- When did it happen?
- Where did it happen?
- Who is responsible?
- Were their actions successful?
- What was the result of their action?

The following hypothetical scenario can illustrate this aspect. Suppose a student allegedly accessed inappropriate content on a University network. By reviewing the logs, a systems administrator could then answer the following:

What happened?
An adversary was confirmed to have accessed SwiftSpend Financial’s GitLab instance.

When did it happen?
Access started at 22:10 on Wednesday, September 8th, 2023.

Where did it happen?
The event originated from a device with an IP address of 10.10.133.168 within the VPN Users’ segment (10.10.133.0/24).

Who is responsible?
Upon examining the network logs, it was observed that the device, identified by the User-Agent “Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/115.0”, was allocated the IP address 10.10.133.168.

Were they successful?
Yes, since an API Key was found to be publicly exposed on the GitLab instance. Moreover, the web proxy logs confirm that the adversary device reached gitlab.swiftspend.finance and maintained access through their uploaded web shell.

What is the result of their action?
The adversary achieved remote code execution on gitlab.swiftspend.finance and performed post-exploitation activities.

**Question 1:** What is the name of your colleague who left a note on your Desktop?

**Question 2:** What is the full path to the suggested log file for initial investigation?

### Task 3: Types, Formats, and Standards

#### Log Types

Logs are essential records of system activity that provide insight into a system’s operation, performance, and security. Common log types include:

- Application Logs: Messages related to application activities.
- Audit Logs: Operations significant for compliance.
- Security Logs: Security-related events like logins and firewall actions.
- Server Logs: Diverse logs generated by servers.
- System Logs: Logs detailing system activities and hardware status.
- Network Logs: Logs about network traffic and related events.
- Database Logs: Records of activities within databases.
- Web Server Logs: Web server activity, such as processed requests.

Understanding these logs is crucial for troubleshooting, performance tuning, and security monitoring.

#### Log Formats

Logs are structured in specific formats, primarily categorized as:

- Semi-structured Logs: Combines structured and unstructured data. Examples include Syslog and Windows Event Log.
- Structured Logs: Follows a strict format, easy for analysis. Examples are CSV, JSON, W3C Extended Log Format, and XML.
- Unstructured Logs: Free-text logs, rich but harder to parse. Examples include NCSA Common and Combined Log Formats.

Custom-defined formats might require specialized tools for interpretation.

#### Log Standards

Log standards set the guidelines for log creation, transmission, and storage. They dictate aspects like format, logging events, security of transmission, and retention duration. Important log standards include Common Event Expression (CEE), OWASP Logging Cheat Sheet, Syslog Protocol, NIST Special Publication 800–92, and various cloud platform guidelines. These standards ensure uniformity and effectiveness in log management practices.

**Question 1:** Based on the list of log types in this task, what log type is used by the log file specified in the note from Task 2?

**Question 2:** Based on the list of log formats in this task, what log format is used by the log file specified in the note from Task 2?

### Task 4: Collection, Management, and Centralization

#### Log Collection

- Focuses on the aggregation of logs from various sources.
- Maintaining accurate system time using Network Time Protocol (NTP) is essential.
- Steps:
  - Identify log sources.
  - Choose a log collector tool.
  - Set collection parameters, ensuring time synchronization with NTP.
  - Test the log collection process.

  Note: While using VMs without internet access, NTP might not be available.

#### Log Management

- Concerned with the secure, organized storage of logs and their efficient retrieval.
- Key steps:
  - Securely store logs, considering retention and access.
  - Organize logs by source, type, etc.
  - Backup logs regularly.
  - Periodically review stored logs.

#### Log Centralization

- Centralizes logs for faster access, in-depth analysis, and quick incident response.
- Steps:
  - Choose a system like Elastic Stack or Splunk for log centralization.
  - Connect all log sources to the system.
  - Monitor logs in real-time and set up alerts.
  - Integrate the central system with other monitoring tools.

Combining these steps ensures comprehensive log collection, management, and centralization.

#### Getting Started with Elastic Stack

1. **Introduction to Elastic Stack:**
   - Elastic Stack, formerly ELK Stack, comprises Elasticsearch, Logstash, and Kibana.
   - Elasticsearch: Search and analytics engine.
   - Logstash: Data
**Log Management with Elastic Stack**

**Getting Started with Elastic Stack:**

1. **Introduction to Elastic Stack:**
   - Elastic Stack, formerly ELK Stack, comprises Elasticsearch, Logstash, and Kibana.
   - Elasticsearch: Search and analytics engine.
   - Logstash: Data processing pipeline.
   - Kibana: Data visualization.

2. **Installation:**
   - Follow the official documentation for installation on your platform (e.g., Windows, Linux).
   - Ensure all components (Elasticsearch, Logstash, Kibana) are correctly installed and running.

3. **Configuring Logstash:**
   - Create a configuration file specifying the log file location and the output to Elasticsearch.
   - Example configuration snippet:
     
     input {
       file {
         path => "/path/to/your/logfile.log"
         start_position => "beginning"
       }
     }
     output {
       elasticsearch {
         hosts => ["localhost:9200"]
         index => "logs-%{+YYYY.MM.dd}"
       }
     }
     ```
   - Save the configuration file and start Logstash.

4. **Setting Up Kibana:**
   - Access Kibana through the web interface (usually http://localhost:5601).
   - Configure Kibana to connect to your Elasticsearch instance.
   - Create index patterns to visualize and analyze your log data.

**Practice:**

Use the Elastic Stack to collect, manage, and visualize logs from your web server scenario. Ensure you can identify and correlate events as per the scenario provided.

**Question 1:** What is the name of the index created in Elasticsearch for the logs?

**Question 2:** What Kibana visualization can be used to analyze the frequency of log events over time?

### Task 5: Analyzing and Interpreting Logs

#### Analysis Techniques

- **Filtering and Searching:** Utilize search queries to find specific log entries or filter out irrelevant data.
- **Pattern Recognition:** Identify recurring patterns that may indicate normal or abnormal behavior.
- **Correlation:** Link related log events to understand the sequence of actions.
- **Anomaly Detection:** Detect deviations from normal patterns to identify potential security incidents.

#### Tools for Log Analysis

- **grep (Linux):** Command-line utility for searching plain-text data.
- **Splunk:** Comprehensive platform for searching, monitoring, and analyzing machine-generated big data.
- **Elastic Stack (ELK):** Suite for searching, analyzing, and visualizing log data.
- **Graylog:** Open-source log management platform.
- **SIEM Solutions:** Security Information and Event Management tools (e.g., IBM QRadar, ArcSight).

**Practice:**

Use the Elastic Stack or another tool of your choice to analyze the log file from the scenario. Identify key events and correlate them to understand the adversary’s actions.

**Question 1:** What was the IP address of the adversary’s device?

**Question 2:** What type of attack was performed by the adversary?

### Task 6: Hands-on Lab - Detecting Adversary Actions

#### Scenario Recap

Revisit the scenario where the SwiftSpend Financial web server was attacked. Use your logging and analysis setup to detect and understand the adversary’s actions.

#### Steps to Follow

1. **Start the VM and access the logs.**
2. **Collect the logs using the configured log collector.**
3. **Analyze the logs to identify suspicious activities.**
4. **Correlate events to trace the adversary’s actions.**
5. **Document your findings and report the adversary’s activities.**

**Questions:**

- **What specific action was taken by the adversary at 22:10 on Wednesday, September 8th, 2023?**
- **What was the outcome of the adversary’s action?**

### Conclusion

Log analysis is a crucial skill for cybersecurity professionals. Understanding logs, their collection, management, and analysis helps in detecting and mitigating potential security incidents. Through hands-on practice, you can enhance your ability to interpret log data and respond to adversary actions effectively.

By mastering log analysis techniques and tools, you are better equipped to maintain a secure and resilient digital environment.
